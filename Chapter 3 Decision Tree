'''
决策树优缺点：
（1）优点:计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
（2）缺点：可能会产生过度匹配问题。
（3）适用数据类型：数值型和标称型。
决策树的一般流程：
（1）收集数据：可以使用任何方法。
（2）准备数据：树构造算法只适用标称型数据，因此数值型数据必须离散化。
（3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
（4）训练算法：构造树的数据结构。
（5）测试算法：使用经验树计算错误率。
（6）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。
'''

#以下代码的文件命名为tree.py
#--------------------------------------------------------计算数据集的熵--------------------------------------------------------------
# 如果待分类的事物可能会出现多个结果x，则第i个结果xi发生的概率为p(xi),那么我们可以由此计算出xi的信息熵为l(xi)=p(xi)log(1/p(xi))=-p(xi)log(p(xi))
#那么，对于所有可能出现的结果，事物所包含的信息希望值（信息熵）就为：H=-Σp(xi)log(p(xi))，i属于所有可能的结果
from math import log

def calcShannonEnt(dataSet):
    numEntries = len(dataSet)  #获取数据集的行数,dataSet.shape(0)也是获取行数
    labelCounts = {}   #设置字典的数据结构
    for featVec in dataSet:  #注意这种在数组中for循环的方法，featVec表示的是每一行的向量
        currentLabel = featVec[-1]  #获取特征向量的最后一列的标签
        if currentLabel not in labelCounts.keys():  #如果不存在keys()关键字
            labelCounts[currentLabel] = 0     #将当前标签/0键值对存入字典中
        labelCounts[currentLabel]+=1   #否则将当前标签对应的键值加1
    ShannonEnt = 0    #初始化熵为0
    for key in labelCounts:    
        prob = (labelCounts[key])/numEntries   #对于数据集中所有的分类类别，计算各个类别出现的频率
        ShanninEnt -=prob*log(prob,2)      #计算各个类别信息期望值 log(prob,2)表示以2为底的prob的对数
    return ShannonEnt
#可以自己创建一个简单的数据集来验证熵的计算是否正确，数据集中包含两个特征'no surfacing','flippers';数据的类标签有两个'yes','no'
def createDataSet():
    dataSet=[[1,1,'yes'],
            [1,1,'yes'],
            [1,0,'no'],
            [0,1,'no'],
            [0,1,'no']]
    labels=['no surfacing','flippers']
    return dataSet,labels  #返回数据集和类标签
#可以在Python命令提示符下输入以下命令，来检测输出的熵的值是否正确，另外需要说明的是，熵越高，那么混合的数据就越多，如果我们在数据集中添加更多的分类，会导致熵结果增大
#>>>import tree.py
#>>>myDat,label=tree.createDataSet()
#>>>tree.calcShannonEnt(myDat)

#-----------------------------------------------按照某一个特征划分数据集------------------------------------------------------------
#分类的标准是按照该特征分类可以获得最大的信息增益
#信息增益的公式为：信息增益H(D,A)=原始数据集的信息熵H(D)-特征A对数据集进行划分后信息熵H(D/A)
def splitDataSet(dataSet,axis,value):    #@dataSet:待划分的数据集 @axis:划分数据集的特征(注意这里输入的列的下标，而不是列) @value:特征的取值
	retDataSet = []   #需要说明的是,python语言传递参数列表时，传递的是列表的引用，如果在函数内部对列表对象进行修改，将会导致列表发生变化，为了不修改原始数据集，创建一个新的列表对象进行操作
	for featVec in dataSet:   #提取数据集的每一行的特征向量
		if featVec[axis] == value:    #如果该特征的取值为value
			reducedFeatVec=featVec[:axis]    #将特征向量的0~axis-1列存入列表reducedFeatVec
			reducedFeatVec.extend(featVec[axis+1:])   #将特征向量的axis+1~最后一列存入列表reducedFeatVec
			#extend()是将另外一个列表中的元素（以列表中元素为对象）添加到当前列表中，比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,4,5,6]
			#append()是将另外一个列表（以列表为对象）添加到当前列表中，比如a=[1,2,3],b=[4,5,6],则a.extend(b)=[1,2,3,[4,5,6]]
			retDataSet.append(reducedFeatVec)
	return retDataSet
	
#----------------------------------------------选择最好的划分数据集的特征---------------------------------------------------------
#使用某一特征划分数据集，信息增益最大，则选择该特征作为最优特征
def chooseBestFeatureToSplit(dataSet):
    numFeatures=len(dataSet[0])-1   #获取数据集特征的数目(不包含最后一列的类标签),dataSet[0]表示选取第一行
    baseEntropy=calShannonEnt(dataSet)     #计算未进行划分的信息熵
    #最优信息增益    最优特征
    bestInfoGain=0.0;bestFeature=-1
    
    for i in range(numFeatures):    #利用每一个特征分别对数据集进行划分，计算信息增益
       
        featList=[example[i] for example in dataSet]   #得到特征i的特征值列表example表示迭代dataSet的每一行，example[i]表示得到每一行第i列的特征值
       
        uniqueVals=set(featList)   #利用set集合的性质--元素的唯一性，得到特征i的取值，比如a=[1,2,1,3,3,4],那么set(a)=(1,2,3,4)
        #信息增益0.0
        newEntropy=0.0
       
        for value in uniqueVals:                      #对特征的每一个取值，分别构建相应的分支
            subDataSet=splitDataSet(dataSet,i,value)  #根据特征i的取值将数据集进行划分为不同的子集,利用splitDataSet()获取特征取值Value分支包含的数据集
            prob=len(subDataSet)/float(len(dataSet))  #计算特征取值value对应子集占数据集的比例
            newEntropy+=prob*calShannonEnt(subDataSet)       #计算占比*当前子集的信息熵,并进行累加得到总的信息熵
			
        #计算按此特征划分数据集的信息增益
        #公式特征A,数据集D
        #则H(D,A)=H(D)-H(D/A)
        infoGain=baseEntropy-newEntropy
		
        #比较此增益与当前保存的最大的信息增益
        if (infoGain>bestInfoGain):
           bestInfoGain=infoGain  #保存信息增益的最大值
           bestFeature=i          #相应地保存得到此最大增益的特征i
        
    return bestFeature            #返回最优特征
			
#-------------------------------------------------采用多数表决的方法完成分类--------------------------------------------------------
#当遍历完所有的特征属性后，类标签仍然不唯一(分支下仍有不同分类的实例)的时候需要用到以下代码，选取多数分类标签为该类标签
def majorityCnt(classList):
    classCount={}  #创建一个类标签的字典
    #遍历类标签列表中每一个元素
    for vote in classList:
    	if vote not in classCount.keys():   #如果元素不在字典中
        	classCount[vote]=0   #在字典中添加新的键值对
        
        classCount[vote]+=1      #否则，当前键对于的值加1
    #对字典中的键对应的值所在的列，按照又大到小进行排序(改方法在KNN算法里面也用到过)
    #@classCount.iteritems 得到列表对象
    #@key=operator.itemgetter(1) 获取列表对象的第二个域的值
    #@reverse=true 降序排序，默认是升序排序
    sortedClassCount=sorted(classCount.iteritems, key=operator.itemgetter(1),reverse=true)
    #返回出现次数最多的类标签
    return sortedClassCount[0][0]

#-----------------------------------------------通过递归的方式构建决策树的代码-------------------------------------------------------
def createTree(dataSet,labels):                        #这里的参数labels指的是各种特征的名称，比如书上的'no surfacing'和'flipper'
    classList=[example[-1] for example in dataSet]     #获取数据集中的最后一列的类标签，存入classList列表
    
    #通过count()函数获取类标签列表中第一个类标签的数目                                                
    if classList.count(classList[0])==len(classList):  #判断数目是否等于列表长度，相同表明所有类标签相同，属于同一类
        return classList[0]
    #遍历完所有的特征属性，此时数据集的列为1，即只有类标签列
    if len(dataSet[0])==1:
        #多数表决原则，确定类标签
        return majorityCnt(classList)
   
    bestFeat=chooseBestFeatureToSplit(dataSet)       #确定出当前最优的分类特征，返回列的下标
    bestFeatLabel=labels[bestFeat]                   #在特征标签列表中获取该特征对应的值，得到特征的名称
    myTree={bestFeatLabel:{}}                        #采用字典嵌套字典的方式，存储分类树信息。该类嵌套对于初学者可能不太熟悉。
                                                     #可以举一个简单的例子如 a={'小红':{'身高':170,'体重':60},'小明':{'身高':175,'体重':70}}
    ##此位置书上写的有误，书上为del(labels[bestFeat])    #这样我们取值或者赋值的时候就可以用 a[0][0]来获得小红的身高值
    ##相当于操作原始列表内容，导致原始列表内容发生改变
    ##按此运行程序，报错'no surfacing'is not in list
    ##以下代码已改正
   
    subLabels=labels[:]                                    #复制当前特征标签列表，防止改变原始列表的内容
    del(subLabels[bestFeat])                               #删除属性列表中当前分类数据集特征
    featValues=[example[bestFeat] for example in dataSet]  #获取数据集中最优特征所在列
    uniqueVals=set(featValues)                             #采用set集合性质，获取特征的所有的唯一取值
	
    #遍历每一个特征取值
    for value in uniqueVals:
        #采用递归的方法利用该特征对数据集进行分类
        #@bestFeatLabel 分类特征的特征标签值
        #@dataSet 要分类的数据集
        #@bestFeat 分类特征的标称值,即特征列的下标
        #@value 标称型特征的取值(即把文本转化成数字表示，如书中用'0'代替'否'，用'1'代替'是')
        #@subLabels 去除分类特征后的子特征标签列表
        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels)
    return myTree
			
		
		
#-----------------------------------------------------利用matplotlib绘制决策树----------------------------------------------------
#以下代码保存文件名为treePlotter.py的文件

import matplotlib.pyplot as plt

#这里是对绘制是图形属性的一些定义，可以不用管，主要是后面的算法
decisionNode = dict(boxstyle="sawtooth", fc="0.8")  #定义了判断节点的图框格式锯齿状，fc=0.8表示框内颜色的深浅
leafNode = dict(boxstyle="round4", fc="0.8")        #定义了叶子节点的图框格式圆状，fc=0.8表示框内颜色的深浅
arrow_args = dict(arrowstyle="<-")                  #定义了箭头的形式

#这是递归计算树的叶子节点个数，比较简单
def getNumLeafs(myTree):
    numLeafs = 0
    firstStr = myTree.keys()[0]                    #myTree.keys()拿到键的列表，[0]即取到第一个分类特征的名称
    secondDict = myTree[firstStr]                  # 得到按该特征分类之后的字典
    for key in secondDict.keys():                  #循环判断字典内的键所对应的值是不是叶子节点
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            numLeafs += getNumLeafs(secondDict[key])      #如果是字典的话就要递归进入字典内部继续判断叶子节点的个数
        else:   numLeafs +=1                              #如果不是字典的话，就是叶子节点，那么叶子节点就加1
    return numLeafs
#这是递归计算树的深度，比较简单，形式如同上面的叶子节点的代码
def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = myTree.keys()[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes
            thisDepth = 1 + getTreeDepth(secondDict[key])
        else:   thisDepth = 1
        if thisDepth > maxDepth: maxDepth = thisDepth
    return maxDepth
#这个是用来以注释形式绘制节点和箭头线，执行一次即完成一次分支的绘制
def plotNode(nodeTxt, centerPt, parentPt, nodeType):   centerPt指箭头头部点的坐标，parentPt指箭头尾部点的坐标
    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',
             xytext=centerPt, textcoords='axes fraction',
             va="center", ha="center", bbox=nodeType, arrowprops=arrow_args )
#这个是用来绘制线上的标注，表示该特征的取值分别是多少，相对简单
def plotMidText(cntrPt, parentPt, txtString):
    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]
    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]
    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)
    
#重点，递归，决定整个树图的绘制，难（自己认为）
def plotTree(myTree, parentPt, nodeTxt):  
    numLeafs = getNumLeafs(myTree)  #得到叶子节点的总数，由于横纵坐标的范围都是从0到1，所以1/numleafs就得到每两个节点的距离
    depth = getTreeDepth(myTree)    #得到总深度，方法同叶子节点数目
    firstStr = myTree.keys()[0]     
    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)
    #cntrPt这个变量记录了递归过程中每一个根节点的横坐标
    #上面的代码需要分开来理解，即plotTree.xOff +  float(numLeafs)/2.0/plotTree.totalW +1.0/2.0/plotTree.totalW
    #我们知道numLeaf记录了每次递归调用时当前树的叶子节点个数，float(numLeafs)/2.0/plotTree.totalW表示得到这些叶子节点的中间点到这些叶子节点两边的距离
    #而后面的1.0/2.0/plotTree.totalW表示整个x轴的中间点距离原点的距离，而plotTree.xOff记录了每一次根节点的位置，它的初始值为-0.5/plotTree.totalW ，是往左偏了0.5/plotTree.tatalW 的，这里正好加回去
    #初始第一次执行时float(numLeafs)和plotTree.totalW是相等的，所以第一次的centerPt和parentPt是相等的，都为(0.5,1),后面递归就作为parentPt了
    
    plotMidText(cntrPt, parentPt, nodeTxt)  
    plotNode(firstStr, cntrPt, parentPt, decisionNode)   #上面到这里的代码都是画的根节点，即特征的名称
    secondDict = myTree[firstStr]
    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD  #画完根节点之后纵坐标深度下移一层
    for key in secondDict.keys():
        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes   
            plotTree(secondDict[key],cntrPt,str(key))        #如果是字典就递归，上一次的centrPt作为下一个的parentPt使用
        else:   #如果是叶子节点就画出来
            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW     #每画一个叶子节点就要向右移动一个1/plotTree.totalW的长度
            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)   #画出叶子节点
            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD    #该语句第一次执行使是已经第一次到达了树的底部，即到达了底部最最左端第一组的叶子节点
    #这里的plotTree.yOff加上一个距离之后，就返回到上一层的根节点处，继续循环向右画出其余根节点和叶子节点。程序是从下向上，从左向右，一层一层依次执行的。

#这个是真正的绘制，上边是逻辑的绘制
def createPlot(inTree):
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    axprops = dict(xticks=[], yticks=[])
    createPlot.ax1 = plt.subplot(111, frameon=False)    #no ticks
    plotTree.totalW = float(getNumLeafs(inTree))
    plotTree.totalD = float(getTreeDepth(inTree))
    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;
    plotTree(inTree, (0.5,1.0), '')
    plt.show()
#这个是用来创建数据集即决策树
def retrieveTree(i):
    listOfTrees =[{'no surfacing': {0:{'flippers': {0: 'no', 1: 'yes'}}, 1: {'flippers': {0: 'no', 1: 'yes'}}, 2:{'flippers': {0: 'no', 1: 'yes'}}}},
                  {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
                  ]
    return listOfTrees[i]
	
	
#---------------------------------------------------------测试算法----------------------------------------------------------------
#以下代码保存在tree.py中
#完成决策树的构造后，采用决策树实现具体应用
#@intputTree 构建好的决策树,即上面的myTree函数
#@featLabels 特征标签列表,即['no surfacing','flipper']
#@testVec 测试实例，即是一个各个特征值的列表，如书中[1,0],'1'代表no surfacing 的值，'0'代表flipper的值
def classify(inputTree,featLabels,testVec):

    firstStr=list(inputTree.keys())[0]     #找到树的第一个分类特征，即每次递归的根节点，如 no surfacing和flipper。注意python2.x和3.x区别，2.x可写成firstStr=inputTree.keys()[0]，而不支持3.x
    secondDict=inputTree[firstStr]         #从树中得到该分类特征的所有分支组成的字典，有0和1
    featIndex=featLabels.index(firstStr)   #根据分类特征的列表的index方法找到对应的标称型数据值，'no surfacing'对应的索引为0
   
    for key in secondDict.keys():          #遍历分类特征所有的取值，no surfacing 有0和1两个键
        if testVec[featIndex]==key:        #如果测试实例的该特征的取值等于该key所代表的取值就进入下一步判断，否则就进入下一个key的循环
            
            if type(secondDict[key]).__name__=='dict':                    #type()函数判断该子节点是否为字典类型
                classLabel=classify(secondDict[key],featLabels,testVec)   #子节点为字典类型，则表明是另一个根节点代表的特征的开始，从该分支树开始继续遍历分类
            else: classLabel=secondDict[key]                              #如果是叶子节点，则返回节点取值
    return classLabel
	
	
#---------------------------------------------------------存储算法于内存中---------------------------------------------------------
#我们知道构建决策树是非常耗时的任务，即使很小的数据集，也要花费几秒的时间来构建决策树，这样显然耗费计算时间。所以，我们可以将构建好的决策树保存在磁盘中，这样当我们需要的时候，再从磁盘中读取出来使用即可。	
#决策树的存储：python的pickle模块序列化决策树对象，使决策树保存在磁盘中
#在需要时读取即可，数据集很大时，可以节省构造树的时间
#pickle模块存储决策树
def storeTree(inputTree,filename):
    #导入pickle模块
    import pickle
    #创建一个可以'写'的文本文件
    #这里，如果按树中写的'w',将会报错write() argument must be str,not bytes
    #所以这里改为二进制写入'wb'
    fw=open(filename,'wb')
    #pickle的dump函数将决策树写入文件中
    pickle.dump(inputTree,fw)
    #写完成后关闭文件
    fw.close()
#取决策树操作    
def grabTree(filename):
    import pickle
    #对应于二进制方式写入数据，'rb'采用二进制形式读出数据
    fr=open(filename,'rb')
    return pickle.load(fr)

#-----------------------------------------------------示例：使用决策树预测隐形眼镜类型------------------------------------------------------
#步骤如下：
#（1）收集数据：文本数据集'lenses.txt'
#（2）准备数据：解析tab键分隔开的数据行
#（3）分析数据：快速检查数据，确保正确地解析数据内容
#（4）训练算法：构建决策树
#（5）测试算法：通过构建的决策树比较准确预测出分类结果
#（6）算法的分类准确类满足要求，将决策树存储下来，下次需要时读取使用

def predictLensesType(filename):
    #打开文本数据
    fr=open(filename)
    #将文本数据的每一个数据行按照tab键分割，并依次存入lenses
    lenses=[inst.strip().split('\t') for inst in fr.readlines()]
    #创建并存入特征标签列表
    lensesLabels=['age','prescript','astigmatic','tearRate']
    #根据继续文件得到的数据集和特征标签列表创建决策树
    lensesTree=createTree(lenses,lensesLabels)
    return lensesTree	

#决策树算法可能或出现的过度匹配（过拟合）的问题，当决策树的复杂度较大时，很可能会造成过拟合问题。
#此时，我们可以通过裁剪决策树的办法，降低决策树的复杂度，提高决策树的泛化能力。比如，如果决策树的某一叶子结点只能增加很少的信息，那么我们就可将该节点删掉，将其并入到相邻的结点中去，这样，降低了决策树的复杂度，消除过拟合问题。
#本章使用的算法称ID3,它是一个很好 的算法但并不完美。ID3算法无法直接处理数值型数据 尽管我们可以通过量化的方法将数值型数据转化为标称型数值，但是如果存在太多的特征划分，ID3算法仍会面临其他问题。	
	
	
	



