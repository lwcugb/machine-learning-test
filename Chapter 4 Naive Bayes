'''
一、基础知识
1 准备知识：条件概率公式

相信学过概率论的同学对于概率论绝对不会陌生，如果一时觉得生疏，可以查阅相关资料，在这里主要是想贴出条件概率的计算公式：
　　P(A|B)=P(A,B)/P(B)=P(B|A)*P(A)/P(B)
2 如何使用条件概率进行分类
　　假设这里要被分类的类别有两类，类c1和类c2，那么我们需要计算概率p(c1|x,y)和p(c2|x,y)的大小并进行比较：
如果：p(c1|x,y)>p(c2|x,y),则(x,y)属于类c1
     p(c1|x,y)<p(c2|x,y),则(x,y)属于类c2
我们知道p(x,y|c)的条件概率所表示的含义为：已知类别c1条件下，取到点(x，y)的概率；那么p(c1|x,y)所要表达的含义呢？显然，我们同样可以按照条件概率的方法来对概率含义进行描述，即在给定点(x,y)的条件下，求该点属于类c1的概率值。那么这样的概率该如何计算呢？显然，我们可以利用贝叶斯准则来进行变换计算：
　　p(ci|x,y)=p(x,y|ci)*p(ci)/p(x,y)
如果特征之间是彼此独立的那么上式可以写成：p(c i |w)=p(w 1 |c i )∗p(w 2 |c i )∗⋅⋅⋅∗p(w n |c i )∗p(c i )/p(w）
接下来对p(c i |w) 取对数，则可是乘法变为加法，上式又可变为: Inp(c i |w)=Inp(w 1 |c i )+Inp(w 2 |c i )+⋅⋅⋅+Inp(w n |c i )+Inp(c i )−Inp(w)
                                   
利用上面的公式，我们可以计算出在给定实例点的情况下，分类计算其属于各个类别的概率，然后比较概率值，选择具有最大概率的那么类作为点(x,y)的预测分类结果。

　　以上我们知道了通过贝叶斯准则来计算属于各个分类的概率值，那么具体而言，就是计算贝叶斯公式中的三个概率，只要得到了这三个概率值，显然我们就能通过贝叶斯算法预测分类的结果了。因此，到了这里，我们就知道了朴树贝叶斯算法的核心所在了。
3 朴素贝叶斯中朴素含义
　　 "朴素"含义：本章算法全称叫朴素贝叶斯算法，显然除了贝叶斯准备，朴素一词同样重要。这就是我们要说的条件独立性假设的概念。条件独立性假设是指特征之间的相互独立性假设，所谓独立，是指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。举个例子来说，假设单词bacon出现在unhealthy后面是与delisious后面的概率相同。当然，我们知道其实并不正确，但这正是朴素一词的含义。同时，朴素贝叶斯另外一个含义是，这些特征同等重要。虽然这些假设都有一定的问题，但是朴素贝叶斯的实际效果却很好。
二、朴素贝叶斯完成文档分类
     朴素贝叶斯的一个非常重要的应用就是文档分类。在文档分类中，整个文档（比如一封电子邮件）是实例，那么邮件中的单词就可以定义为特征。说到这里，我们有两种定义文档特征的方法。一种是词集模型，另外一种是词袋模型。顾名思义，词集模型就是对于一篇文档中出现的每个词，我们不考虑其出现的次数，而只考虑其在文档中是否出现，并将此作为特征；假设我们已经得到了所有文档中出现的词汇列表，那么根据每个词是否出现，就可以将文档转为一个与词汇列表等长的向量。而词袋模型，就是在词集模型的基础上，还要考虑单词在文档中出现的次数，从而考虑文档中某些单词出现多次所包含的信息。
     好了，讲了关于文档分类的特征描述之后，我们就可以开始编代码，实现具体的文本分类了
'''

#以下代码保存为bayes.py     
#-------------------------------------------------------从文本中构建词条向量-----------------------------------------------------------------
#1 要从文本中获取特征，需要先拆分文本，这里特征是指来自文本的词条，每个词条是字符的任意组合。词条可以理解为单词，当然也可以是非单词词条，比如URL，IP地址或者其他任意字符串 
# 将文本拆分成词条向量后，将每一个文本片段表示为一个词条向量，值为1表示出现在文档中，值为0表示词条未出现
from numpy import *
def loadDataSet():  #该函数简单不用多说
    postingList = [['my','dog','has','flea',\
                  'problems','help','please'],
                 ['maybe','not','take','him',\
                  'to','dog','park','stupid'],
                 ['my','dalmation','is','so','cute',
                  'I','love','him'],
                 ['stop','posting','stupid','worthless','garbage'],
                 ['my','licks','ate','my','steak','how',\
                  'to','stop','him'],
                 ['quit','buying','worthless','dog','food','stupid']]
    classVec = [0,1,0,1,0,1]
    return postingList, classVec
    
#统计所有文档中出现的词条列表
def createVocabList(dataSet):
    vocabSet=set([])                            #新建一个存放词条的集合
    for document in dataSet:
        vocabSet = vocabSet | set(document)     #将文档列表转为集合的形式，保证每个词条的唯一性.然后与vocabSet取并集（注意并集符号的使用方法），向vocabSet中添加没有出现的新词条
    return list(vocabList)                      #再将集合转化为列表，便于接下来的处理

#根据词条列表中的词条是否在文档中出现(出现1，未出现0)，将文档转化为词条向量 
def setOfWords2Vec(vocabSet,inputSet):
    returnVec=[0]*len(vocabSet) #新建一个长度为vocabSet的列表，并且各维度元素初始化为0（注意学习这种用乘号来扩充列表元素的方法）
    for word in inputSet:   
        if word in vocabSet:    #如果输入的词条在已有的词条列表中出现
            returnVec[vocabSet.index(word)]=1   #通过列表获取当前word在已有的词条列表中的索引(下标)，并将词条向量中的对应下标的项由0改为1
        else:
            print('the word: %s is not in my vocabulary! '%'word')
    return returnVec            #返回inputet转化后的词条向量
    
#需要说明的是，上面函数creatVocabList得到的是所有文档中出现的词汇列表，列表中没有重复的单词，每个词是唯一的。
    

#------------------------------------------------------用已知分类的文档训练模型-----------------------------------------------------
#训练算法，从词条向量计算概率p(w0|ci),p(w1|ci)...及p(ci) 
def trainNB0(trainMatrix,trainCategory): #@trainMatrix：由每篇文档的词条向量组成的文档矩阵，每个向量的词条数目是相等的；@trainCategory:每篇文档的类标签组成的向量，即是侮辱性的文档记为1，非侮辱性的文档记为0
    numTrainDocs=len(trainMatrix)        #得到文档的总个数，记为矩阵的行数
    numWords=len(trainMatrix[0])         #得到每一个词条向量中词条的个数
    pAbusive=sum(trainCategory)/float(numTrainDocs)  #得到p(c1)的值，因为本例子只有两个分类，p(c0)=1-p(c1)，如果更多的分类需要稍作修改
    p0Num=zeros(numWords);p1Num=zeros(numWords)      #初始化分子的值，即为一个长度与词条向量相等的0向量，下面还要修改
    p0Denom=0.0;p1Denom=0.0                          #初始化分母的值，即为所有词条出现数目的和，下面还要修改
    for i in range(numTrainDocs):
        if trainCategory[i]==1:
            p1Num+=trainMatrix[i]                    #得到所有侮辱性文档中每个词条数目对应的和
            p1Denom+=sum(trainMatrix[i])             #得到所有侮辱性文档中所有词条数目的和
        else:
            p0Num+=trainMatrix[i]                    #得到所有非侮辱性文档中每个词条数目对应的和
            p0Denom+=sum(trainMatrix[i])             #得到所有非侮辱性文档中所有词条数目的和
    p1Vect=p1Num/p1Denom                             #得到侮辱性文档每个特征的概率组成的向量，即 p(w0|c1),p(w1|c1)...
    p0Vect=p0Num/p0Denom                             #得到非侮辱性文档每个特征的概率组成的向量，即 p(w0|c0),p(w1|c0)...
    return p0Vect,p1Vect,pAbusive

'''
!!!针对算法的部分改进

(1)计算概率时，需要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w0|ci)*p(w1|ci)*...p(wN|ci)，然后当其中任意一项的值为0，那么最后的乘积也为0.
为降低这种影响，采用拉普拉斯平滑，在分子上添加a(一般为1)，分母上添加k*a(k表示类别总数)，即在这里将所有词的出现数初始化为1，并将分母初始化为2*1=2

#p0Num=ones(numWords);p1Num=ones(numWords)
#p0Denom=2.0;p1Denom=2.0

(2)解决下溢出问题

　　  正如上面所述，由于有太多很小的数相乘。计算概率时，由于大部分因子都非常小，最后相乘的结果四舍五入为0,造成下溢出或者得不到准确的结果.
  所以，我们可以对成绩取自然对数，即求解对数似然概率。这样，可以避免下溢出或者浮点数舍入导致的错误。同时采用自然对数处理不会有任何损失。

#p0Vect=log(p0Num/p0Denom);p1Vect=log(p1Num/p1Denom)

'''

 
#--------------------------------------------------------------测试算法-----------------------------------------------------------
def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):#@vec2Classify:待测试分类的词条向量；@p0Vec:类别0所有文档中各个词条出现的频数p(wi|c0)；@p0Vec:类别1所有文档中各个词条出现的频数p(wi|c1)；@pClass1:类别为1的文档占文档总数比例
    p1=sum(vec2Classify*p1Vec)+log(pClass1)      #根据朴素贝叶斯分类函数分别计算待分类文档属于类1和类0的概率
    p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)  #在这里不明白为什么要用把两个向量乘起来？？？？？？？不是应该用减吗？？？？？？？
    if p1>p0:
        return 1
    else:
         return 0
    
#分类测试整体函数
def testingNB():
    listOPosts,listClasses=loadDataSet()    #由数据集获取文档矩阵和类标签向量
    myVocabList=createVocabList(listOPosts) #统计所有文档中出现的词条，存入词条列表
    trainMat=[]    #创建新的列表，用于存储训练矩阵
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList,postinDoc)) #将每篇文档利用words2Vec函数转为词条向量，存入文档矩阵中
    p0V,p1V,pAb=trainNB0(array(trainMat),array(listClasses))   #将文档矩阵和类标签向量转为NumPy的数组形式，方便接下来的概率计算。调用训练函数，得到相应概率值
    
    testEntry=['love','my','dalmation']  #测试文档
    thisDoc=array(setOfWords2Vec(myVocabList,testEntry))  #将测试文档转为词条向量，并转为NumPy数组的形式
    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))  #利用贝叶斯分类函数对测试文档进行分类并打印
    
    testEntry1=['stupid','garbage']     #第二个测试文档
    thisDoc1=array(setOfWords2Vec(myVocabList,testEntry1)) #将测试文档转为词条向量，并转为NumPy数组的形式
    print(testEntry1,'classified as:',classifyNB(thisDoc1,p0V,p1V,pAb))

'''
这里需要补充一点，上面也提到了关于如何选取文档特征的方法，上面用到的是词集模型，即对于一篇文档，将文档中是否出现某一词条作为特征，即特征只能为0不出现或者1出现；
然后，一篇文档中词条的出现次数也可能具有重要的信息，于是我们可以采用词袋模型，在词袋向量中每个词可以出现多次，这样，在将文档转为向量时，每当遇到一个单词时，它会增加词向量中的对应值
'''
#具体将文档转为词袋向量的代码为:

def bagOfWords2VecMN(vocabList,inputSet):
    returnVec=[0]*len(vocabList)  #词袋向量
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)]+=1  #某词每出现一次，次数加1
     return returnVec
    
 
#----------------------------------------------------------实例1过滤垃圾邮件-----------------------------------------------------
#处理数据长字符串
#1 用正则表达式对长字符串进行分割，分隔符为除单词和数字之外的任意符号串
#2 将分割后的字符串中所有的大些字母变成小写lower(),并且只保留单词长度大于3的单词  
def testParse(bigString):
    import re
    listOfTokens=re.split(r'\W*',bigString)
    return [tok.lower() for tok in listOPosts if len(tok)>2]
def spamTest():
    docList=[];classList=[];fullTest=[]  #新建三个列表
    for i in range(1,26): #因为TXT的个数有25个
        wordList=testParse(open('email/spam/%d.txt' %i).read())#打开并读取指定目录下的本文中的长字符串，并进行处理返回
        docList.append(wordList)#将得到的字符串列表添加到docList
        fullTest.extend(wordList)#将字符串列表中的元素添加到fullTest
        classList.append(1)#类列表添加标签1
        wordList=testParse(open('email/ham/&d.txt' %i).read()) #打开并取得另外一个类别为0的文件，然后进行处理
        wordList=testParse(open('email/ham/&d.txt' %i).read())
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(0)
    vocabList=createVocabList(docList)#将所有邮件中出现的字符串构建成字符串列表
    #下面的代码和书上稍有不同，因为在3.x版本，range(50)对象无法进行删除操作，所以采用别的方法构造列表
    trainingSet=[x for x in range(50)];testSet=[]#构建一个大小为50的整数列表和一个空列表
    for i in range(10):#随机选取0~49中的10个数，作为索引，构建测试集
        randIndex=int(random.uniform(0,len(trainingSet)))#随机选取0~49中的一个整型数
        testSet.append(trainingSet[randIndex])#将选出的数的列表索引值添加到testSet列表中
        del(trainingSet[randIndex])#从整数列表中删除选出的数，防止下次再次选出。同时将剩下的作为训练集
    
    trainMat=[];trainClasses=[]#新建两个列表，分别存放训练矩阵和类标签向量
    for docIndex in trainingSet:#遍历训练集中的每个字符串列表
        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))#将字符串列表转为词条向量，然后添加到训练矩阵中。书中有错误，应该把fullTest换成docList
        trainClasses.append(classList[docIndex])#将该邮件的类标签存入训练类标签列表中
    
    p0V,p1V,pSpam=trainNB0(array(trainMat),array(trainClasses))#计算贝叶斯函数需要的概率值并返回
    errorCount=0
    for docIndex in testSet:#遍历测试集中的字符串列表
        wordVector=setOfWords2Vec(vocabList,docList[docIndex])#同样将测试集中的字符串列表转为词条向量
        if classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]：#对测试集中字符串向量进行预测分类，分类结果不等于实际结果
            errorCount+=1
        print('the error rate is:',float(errorCount)/len(testSet))
        
        
#代码中，采用随机选择的方法从数据集中选择训练集，剩余的作为测试集。这种方法的好处是，可以进行多次随机选择，得到不同的训练集和测试集。
#从而得到多次不同的错误率，我们可以通过多次的迭代，求取平均错误率，这样就能得到更准确的错误率。这种方法称为留存交叉验证。

#-------------------------------------------------实例2从个人广告中获取区域倾向------------------------------------------------------
#在本例中，我们通过从不同的城市的RSS源中获得的同类型广告信息，比较两个城市人们在广告用词上是否不同。如果不同，那么各自的常用词是哪些？
#而从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解？如果能得到这些信息，分析过后，相信对于广告商而言具有不小的帮助。

#利用RSS源得到文本数据，Universal Feed Parser是Python中最常见的RSS程序库。通过如下语句可以获得相应的文本数据.
import feedparser #需要预先下载这个包，就像安装其他的包一样在命令行中输入 pip install feedparser
ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')
ny['entries']
len(ny['entries'])

#RSS源分类器及高频词去除函数
def calMostFreq(vocabList,fullTest):
    import operator#导入操作符
    freqDict={}#创建新的字典用于存储高频词（键）和出现的次数（值）
    for token in vocabList:
        freqDict[token]=fullTest.count(token) #将单词/单词出现的次数作为键值对存入字典，注意学习fullTest.count(token) 来获得列表中元素出现次数的方法
        sortedFreq=sorted(freqDict.iteritems(),keys=operator.itemgetter(1),reverse=true)#按照键值value(词条出现的次数)对字典进行排序，由大到小
        return sortedFreq[:30]#返回出现次数最多的前30个单词
        
def localWords(feed1,feed0):
    import feedparser
    docList=[];classList=[];fullTest=[]#新建三个列表
    minLen=min(len(feed1['entries']),len(feed0['entries']))#获取条目较少的RSS源的条目数
    for i in range(minLen):
        wordList=textParse(feed1['entries'][i]['summary'])#在这里要清楚feed1['entries']的结构，它是由许多字典元素组成的列表。这里是获得条目中的summary键下的文本
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(1)
        wordList=testParse(feed0['entries'][i]['summary'])
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(0)
    vocabList=createVocabList(docList)#构建出现的所有词条列表
    top30Words=calMostFreq(vocabList,fullTest)#找到出现的单词中频率最高的30个单词
    
    for pairW in top30Words:#遍历每一个高频词，并将其在词条列表中移除。这里移除高频词后错误率下降，如果继续移除结构上的辅助词，错误率很可能会继续下降
        if pairW[0] in vocabList: #注意top30Words的结构是由键值对构成相应的列表，然后由这些列表再构成一个总列表
            vocabList.remove(pairW[0])
    #下面内容与函数spamTest完全相同，就是改变了测试的数目和最后返回后面要用到的值
    trainingSet=[x for x in range(2*minLen)];testSet=[]#构建一个大小为50的整数列表和一个空列表
    for i in range(20):#随机选取0~19中的20个数，作为索引，构建测试集
        randIndex=int(random.uniform(0,len(trainingSet)))#随机选取0~19中的一个整型数
        testSet.append(trainingSet[randIndex])#将选出的数的列表索引值添加到testSet列表中
        del(trainingSet[randIndex])#从整数列表中删除选出的数，防止下次再次选出。同时将剩下的作为训练集
    
    trainMat=[];trainClasses=[]#新建两个列表，分别存放训练矩阵和类标签向量
    for docIndex in trainingSet:#遍历训练集中的每个字符串列表
        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))#将字符串列表转为词条向量，然后添加到训练矩阵中。书中有错误，应该把fullTest换成docList
        trainClasses.append(classList[docIndex])#将该邮件的类标签存入训练类标签列表中
    
    p0V,p1V,pSpam=trainNB0(array(trainMat),array(trainClasses))#计算贝叶斯函数需要的概率值并返回
    errorCount=0
    for docIndex in testSet:#遍历测试集中的字符串列表
        wordVector=setOfWords2Vec(vocabList,docList[docIndex])#同样将测试集中的字符串列表转为词条向量
        if classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]：#对测试集中字符串向量进行预测分类，分类结果不等于实际结果
            errorCount+=1
        print('the error rate is:',float(errorCount)/len(testSet))
        return vocabList,p0V,p1V
        
#需要说明的是，这里用到了将出现次数最多的30个单词删除的方法，结果发现去掉了这些最常出现的高频词后，错误率大幅度上升，这表明了文本中的小部分高频单词占据了文本中绝大部分的用词。
#产生这种情况的原因是因为语言中大部分是冗余和结果辅助性内容。所以，我们不仅可以尝试移除高频词的方法，还可以进一步从某个预定词表(停用词表)中移除结构上的辅助词，通过移除辅助性词，分类错误率会所有下降。
#此外，为了得到错误率的精确估计，应进行多次上述实验，从而得到错误率平均值。     


#对得到的数据进行分析
#最具表征性的词汇显示函数
def getTopWords(ny,sf):
    import operator
    vocabList,p0V,p1V=localWords(ny,sf)#利用RSS源分类器获取所有出现的词条列表，以及每个分类中每个单词出现的概率
    topNY=[];topSF=[]#创建两个元组列表(即列表内的元素是元组组成的)
    for i in range(len(p0V)):  #遍历每个类中各个单词的概率值
        if(p0V[i]>-6.0):topSF.append((vocabList[i],p0V[i]))#往相应元组列表中添加概率值大于阈值的单词及其概率值组成的二元列表
        if(p1V[i]>-6.0):topNY.append((vocabList[i],p1V[i]))#往相应元组列表中添加概率值大于阈值的单词及其概率值组成的二元列表
    #对列表按照每个二元列表中的概率值项进行排序，排序规则由大到小    
    sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=true)#注意lambda这个单行函数规定了用于比较的是概率，而不是名称
    print('SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**')
    for item in sortedSF: #遍历列表中的每一个二元条目列表
        print(item[0])    #打印每个二元列表中的单词字符串元素
    #下面代码解释同上    
    sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=true)
    print('NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**')
    for item in sortedSF:
        print(item[0])
#------------------------------------------------------------小总结---------------------------------------------------------------       
#尽管朴素贝叶斯的条件独立性假设存在一定的问题，但是朴素贝叶斯算法仍然能取得比较理想的分类预测结果。    
#此外，朴素贝叶斯在数据较少的情况下仍然适用，虽然例子中为两类类别的分析，但是朴素贝叶斯可以处理多分类的情况。
#朴素贝叶斯的一个不足的地方是，对输入的数据有一定的要求，需要花费一定的时间进行数据的处理和解析。朴素贝叶斯中用来计算的数据为标称型数据，我们需要将字符串特征转化为相应的离散值，用于后续的统计和计算。
    
        
    
     
