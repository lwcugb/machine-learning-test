
#-----------------------------------------------------KNN算法实现----------------------------------------------------------------

from numpy import *
import operator

#书中例子的数据集
def CreateDataSet():
	group  = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
	labels = ['A','A','B','B']
	return group, labels

def classify0(inX, dataSet, labels, k):
	dataSetSize = dataSet.shape[0] #dataSet是一个数组，shape[0]表示获得数组的行数，shape[1]表示获得列数
	diffMat =  tile(inX,(dataSetSize,1)) - dataSet  #dataSetSize是一个数字，tile()表示用inX构造一个dataSetSize行，1列的数组
	sqDiffMat = diffMat **2
	sqDistances = sqDiffMat.sum(axis=1)   #axis=1表示数组按列求和
	distances = sqDistances ** 0.5
	sortedDistances = distances.argsort()  #注意这里的argsort表示得到是分类之后的下标，而不是元素本身
	classCount={}
	for i in range(k):
		voteIlabel = labels[sortedDistances[i]]   #得到下标对应的labels类
		classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1   #字典的get函数表示得到对应键的值，后面的0，表示没有,该键之后的返回值是0
	sortedClassCount=sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
	return sortedClassCount[0][0]  #得到的对象是一个以元组为元素的list,类似于[('A',2),('B'1)]
	
	 #sorted 函数的使用，第一个参数表示一个要排序的对象(这个对象一定要是可迭代的，否则会出错)，这里的字典的iteritems方法得到的是字典元素的迭代对象
	 #第二个参数key表示要根据那个量进行大小的比较，operator.itemgetter(1)函数表示根据对象的第二个阈值进行比较，reverse=True表示按照从大到小的顺序进行比





#---------------------------------------------------KNN算法实例-------------------------------------------------------------------
#---------------------------------------------------约会网站配对-------------------------------------------------------------------
#---------------------------------------1将text文本数据转化为分类器可以接受的格式-----------------------------------------------------
def file2matrix(filename):
    #打开文件
    fr=open(filename)
    #读取文件每一行到array0Lines列表
    #read():读取整个文件，通常将文件内容放到一个字符串中
    #readline():每次读取文件一行，当没有足够内存一次读取整个文件内容时，使用该方法
    #readlines():读取文件的每一行，组成一个字符串列表，内存足够时使用
    array0Lines=fr.readlines()
    #获取字符串列表行数行数
    numberOfLines=len(array0Lines)
    #建立一个和数据样本等大的0矩阵
    returnMat=zeros((numberOfLines,3))
    #list存储类标签
    classLabelVector=[]
    index=0
    for line in array0Lines:
        #去掉字符串头尾的空格，类似于Java的trim()
        line=line.strip()
        #将整行元素按照tab分割成一个元素列表
        listFromLine=line.split('\t')
        #将listFromLine的前三个元素依次存入returnmat的index行的三列，第一个参数表示行，第二个参数表示列的范围
        returnMat[index,:]=listFromLine[0:3]
        #python可以使用负索引-1表示列表的最后一列元素，从而将标签存入标签向量中
        #使用append函数每次循环在list尾部添加一个标签值
        classLabelVector.append(int(listFromLine[-1]))
        index+=1
    return returnMat,classLabelVector

#----------------------------------------------2准备数据：归一化------------------------------------------------------------
#计算欧式距离时，如果某一特征数值相对于其他特征数值较大，那么该特征对于结果影响要
#远大于其他特征，然后假设特征都是同等重要，即等权重的，那么可能某一特征对于结果存在严重影响

def autonorm(dataSet):
	minVals = dataSet.min(0)  #参数0按列选取最小值，如果换成1表示按行选取最小值
	maxVals = dataSet.max(0)  #参数0按列选取最大值，如果换成1表示按行选取最大值
	ranges  = maxVals-minVals
	normDataSet = zeros(shape(dataSet)) #创建与dataSet等大小的归一化矩阵，shape()获取dataSet的矩阵大小，即行数和列数
	m = dataSet.shape[0]  #获得dataSet的行数， shape[1]表示获取列数
	normDataSet = dataSet - tile(minVals,(m,1))  #将dataSet的每一行的对应列减去minVals中对应列的最小值
	normDataSet = normDataSet/tile(ranges,(m,1)) #归一化，公式newValue=(value-minvalue)/(maxVal-minVal)，还要注意这里的 '/'表示每一个矩阵内的数分别相除，而不是矩阵相除
	return normDataSet, ragnes, minVals

#------------------------------------------------3测试算法----------------------------------------------------------------------
def datingClassTest():
	hoRatio=0.1 #设定测试的样本比例，不同的占比可能得到的错误率都会有所差别
	datingDataMat, datingLabels = file2matrix('datingDataTest2')  #从文本中提取得到数据特征，及对应的标签
	normMat,ranges,minVals = aotuNorm(datingDataMat) #对数据特征进行归一化
	m = normMat.shape[0] #得到行数
	numTestVecs = int(m*hoRatio) #测试样本数量
	errorCount = 0.0 #错误数据初始化
	for i in range(numTestVecs):
		classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) #利用分类函数classify0获取测试样本数据分类结果
		print("the classifier came back with: %d, the real answer is: %d" %(classifierResult,datingLabels[i])) #打印预测结果和实际标签
		if(classifierResult != datingLabels[i]):errorCount+=1.0 #如果预测输出不等于实际标签,错误数增加1.0
	print("the total error rate is: %f" %(errorCount/float(numTestVecs))) #打印最后的误差率
	
#------------------------------------------4运用已经完成的算法进行实际分类---------------------------------------------------
def classifyPerson():
	resultList = ['not at all','in small doses','in large doses']  #结果列表，对应的是datingTestSet2里面最后一列的1,2,3
	percentTats = float(raw_input("percentage of time spent playing video games?")) #在python3.x中，已经删除raw_input()，取而代之的是input(),另外输入的是文本类型的数据，要用float去转换
	ffMiles = float(raw_put("frequent filer miles earned per year?"))
	iceCream=float(input("liters of ice cream consumed per year?"))
	datingDataMat,datingLabels=file2matrix('datingTestSet2.txt')   #获得数据
	normMat,ranges,minValues=autoNorm(datingDataMat)  #归一化数据
	inArr=array([ffMiles,percentTats,iceCream])  #将输入的数据变成数组形式
	classifierResult = classify0((inArray-minVals)/ranges,norMat,datingLabels,3)
	print("You will probably like this person:", resultList[classifierResult-1]) #因为这里的labels是数字1,2,3，所以变成list的下标要减一
	
	
#--------------------------------------------------------------knn算法实例------------------------------------------------------
#--------------------------------------------------------------手写识别系统------------------------------------------------------
#---------------------------------------------------------1 将图像转化为测试向量--------------------------------------------------

#图像大小32*32，转化为1*1024的向量
def img2vector(filename):
    returnVec=zeros((1,1024))
    fr=open(filename)
    for i in range(32):
        lineStr=fr.readline()  #每次读取一行
        for j in range(32):
        	returnVec[0,32*i+j]=int(lineStr[j]) #通俗讲：就是根据首地址(位置)的偏移量计算出当前数据存放的地址(位置)
    return returnVec
	
#--------------------------------------------------------------2 测试代码------------------------------------------------------------
def handWritingClassTest():
	hwLabels = []  #建立存储类型labels的列表
	trainingFileList = listdir('knn/trainingDigits') #列出给定目录的文件名列表，返回格式是list，使用前需导入from os import listdir
	m = len(trainingFileList) #获得文件的行数，即文件的个数，从而确定训练矩阵的行数
	trainingMat = zeros((m,1024))  #创建一个m*1024的矩阵用于存储训练数据
	for i in range(m):
		fileNameStr = trainingFileList[i]
		fileStr = fileNameStr.split('.')[0]   #将字符串按照'.'分开，并将前一部分放于fileStr
		classnumStr = int(fileStr.split('_')[0])  #将fileStr按照'_'分开，并将前一部分存于classNumStr
		hwLabels.append(classNumStr) #得到类型labels的向量
		trainingMat[i,:]=img2vector('knn/trainingDigits/%s' %fileNameStr) #解析目录中的每一个文件，将图像转化为向量，最后存入训练矩阵中
	
	testFileList = listdir('knn/testDigits')  #读取测试数据目录中的文件列表
	errorCount = 0.0
	mTest=len(testFileList)
	for i in range(mTest):
        fileNameStr=testFileList[i]  #获取第i行的文件名
        fileStr=fileNameStr.split('.')[0]  #将字符串按照'.'分开，并将前一部分放于fileStr
        classNumStr=int(fileStr.split('_')[0])  #将fileStr按照'_'分开，并将前一部分存于classNumStr
        vectorUnderTest=img2vector('knn/testDigits/%s' %fileNameStr)  #解析目录中的每一个文件，将图像转化为向量
        classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,3)  #分类预测
        print("the classifierResult came back with: %d,the real answer is: %d" %(classifierResult,classNumStr))  #打印预测结果和实际结果
        if(classifierResult!=classNumStr):errorCount+=1.0  #预测错误，错误数加1次
        print("\nthe total number of errors is: %d" %errorCount)   #打印错误数和错误率
        print("\nthe total error rate is: %f" %(errorCount/float(mTest)))
	
	
#-------------------------------------------------------------总结---------------------------------------------------------------
'''
k近邻算法的一般流程：
（1）收集数据：可以采用公开的数据源

（2）准备数据：计算距离所需要的数值

（3）分析数据：剔除垃圾信息

（4）测试算法：计算错误率

（5）使用算法：运用在实际中，对实际情况进行预测

K近邻算法的注意事项：

（1）如果我们改变训练样本的数目，调整相应的k值，都会对最后的预测错误率产生影响，我们可以根据错误率的情况，对这些变量进行调整，从而降低预测错误率

（2）k近邻算法的优缺点：

	k近邻算法具有精度高，对异常值不敏感的优点

	k近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离，实际使用时也可能会非常耗时

此外，k近邻算法无法给出数据的基础结构信息，因此我们无法知道平均实例样本和典型实例样本具有怎样的特征。

'''
 	
	
	
	
	
